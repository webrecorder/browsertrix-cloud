{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the Browsertrix official user guide and developer docs. These docs will contain the following sections.</p> <ul> <li>Deployment Guide \u2014 How to install and deploy Browsertrix on your local machine, or in the cloud.</li> <li>Developer Docs \u2014 Information on developing Browsertrix itself.</li> <li>User Guide \u2014 Instructions and reference for using Browsertrix.</li> </ul> <p>If you are unfamiliar with Browsertrix, please check out our website, or the main repository at https://github.com/webrecorder/browsertrix</p> <p>If something is missing, unclear, or seems incorrect, please open an issue and we'll try to make sure that your questions get answered here in the future!</p>"},{"location":"deploy/","title":"Deploying Browsertrix","text":"<p>Browsertrix is designed to be a cloud-native application running in Kubernetes.</p> <p>However, despite the name, it is perfectly reasonable (and easy!) to deploy Browsertrix locally using one of the many available local Kubernetes options.</p> <p>The main requirements for Browsertrix are:</p> <ul> <li>A Kubernetes Cluster</li> <li>Helm 3 (package manager for Kubernetes)</li> </ul> <p>We have prepared a Local Deployment Guide which covers several options for testing Browsertrix locally on a single machine, as well as a Production (Self-Hosted and Cloud) Deployment guide to help with setting up Browsertrix in different production scenarios. Information about configuring storage, crawler channels, and other details in local or production deployments is in the Customizing Browsertrix Deployment Guide.</p>"},{"location":"deploy/customization/","title":"Customizing Browsertrix Deployment","text":"<p>Local and production deployments alike can be customized by modifying the <code>chart/values.yaml</code> Helm chart file or a local override. For more on using local overrides, see the Local Deployment Guide. The remainder of this guide covers some of the customization options available in the Helm chart.</p>"},{"location":"deploy/customization/#default-organization","title":"Default Organization","text":"<p>The <code>default_org</code> setting is used to specify the name for the default organization created in a Browsertrix deployment. A slug will be auto-generated based on this value and can be modified in Org Settings within the application.</p>"},{"location":"deploy/customization/#superuser","title":"Superuser","text":"<p>The <code>superuser</code> setting is used to set the username and password for a deployment's superuser. If <code>password</code> is left blank, the application will auto-generate a secure password for the superuser.</p>"},{"location":"deploy/customization/#crawler-channels","title":"Crawler Channels","text":"<p>The <code>crawler_channels</code> setting is used to specify the Crawler Release Channel option available to users via dropdown menus in workflows and browser profiles. Each crawler channel has an id and a Docker image tag. These channels are modifiable with the restriction that there must always be one channel with the id <code>default</code>. By default this is the only channel available on deployments:</p> <pre><code>crawler_channels:\n  - id: default\n    image: \"docker.io/webrecorder/browsertrix-crawler:latest\"\n</code></pre> <p>This can be extended with additional channels. For example, here is what the value would look like adding a new x.y.z release of Browsertrix Crawler with the id <code>testing</code>:</p> <pre><code>crawler_channels:\n  - id: default\n    image: \"docker.io/webrecorder/browsertrix-crawler:latest\"\n  - id: testing\n    image: \"docker.io/webrecorder/browsertrix-crawler:x.y.z\"\n</code></pre>"},{"location":"deploy/customization/#storage","title":"Storage","text":"<p>The <code>storage</code> setting is used to specify primary and replica storage for a Browsertrix deployment. All configured storage options must be S3-compatible buckets. At minimum, there must be one configured storage option, as can be seen in the default configuration:</p> <pre><code>storages:\n  - name: \"default\"\n    type: \"s3\"\n    access_key: \"ADMIN\"\n    secret_key: \"PASSW0RD\"\n    bucket_name: *local_bucket_name\n\n    endpoint_url: \"http://local-minio.default:9000/\"\n</code></pre> <p>It is possible to add one or more replica storage locations. If replica locations are enabled, all stored content in the application will be automatically replicated to each configured replica storage location in background jobs after being stored in the default primary storage. If replica locations are enabled, at least one must be set as the default replica location for primary backups. This is indicated with <code>is_default_replica: True</code>. If more than one storage location is configured, the primary storage must also be indicated with <code>is_default_primary: True</code>.</p> <p>For example, here is what a storage configuration with two replica locations, one in another bucket on the same Minio S3 service as primary storage as well as another in an external S3 provider:</p> <pre><code>storages:\n  - name: \"default\"\n    type: \"s3\"\n    access_key: \"ADMIN\"\n    secret_key: \"PASSW0RD\"\n    bucket_name: *local_bucket_name\n\n    endpoint_url: \"http://local-minio.default:9000/\"\n    is_default_primary: True\n\n  - name: \"replica-0\"\n    type: \"s3\"\n    access_key: \"ADMIN\"\n    secret_key: \"PASSW0RD\"\n    bucket_name: \"replica-0\"\n\n    endpoint_url: \"http://local-minio.default:9000/\"\n    is_default_replica: True\n\n  - name: \"replica-1\"\n    type: \"s3\"\n    access_key: \"accesskey\"\n    secret_key: \"secret\"\n    bucket_name: \"replica-1\"\n\n    endpoint_url: \"http://s3provider.example.com\"\n</code></pre>"},{"location":"deploy/customization/#horizontal-autoscaling","title":"Horizontal Autoscaling","text":"<p>Browsertrix also includes support for horizontal auto-scaling for both the backend and frontend pods. The auto-scaling will start a new pod when memory/cpu utilization reaches the thresholds.</p> <p>To use auto-scaling, the metrics-server cluster add-on is required. Many k8s provides include metrics server by default, others, like MicroK8S, make it available as an add-on.</p> <p>To enable auto-scaling, set <code>backend_max_replicas</code> and/or <code>frontend_max_replicas</code> to a value &gt;1.</p> <pre><code>backend_max_replicas: 2\n\nfrontend_max_replicas: 2\n</code></pre> <p>By default, the auto-scaling uses the following thresholds for deciding when to start a new pod can also be modified. The default values are:</p> <pre><code>backend_avg_cpu_threshold: 80\n\nbackend_avg_memory_threshold: 95\n\nfrontend_avg_cpu_threshold: 80\n\nfrontend_avg_memory_threshold: 95\n</code></pre>"},{"location":"deploy/customization/#email-smtp-server","title":"Email / SMTP Server","text":"<p>Browsertrix sends user invitations, password resets, background job failure notifications, and other important messages via email. The <code>email</code> setting can be used to configure the SMTP server used to send emails. To avoid email messages from Browsertrix being flagged as spam, be sure to use the same domain for <code>sender_email</code> and <code>reply_to_email</code>.</p>"},{"location":"deploy/customization/#customizing-email-templates","title":"Customizing Email Templates","text":"<p>It is also possible to custom the HTML/plain-text email templates that Browsertrix sends out with a custom <code>--set-file</code> parameter for <code>email.templates.&lt;TEMPLATE_NAME&gt;</code> pointing to an alternate template file. For example, to use a custom <code>invite.html</code> for the invite template, add:</p> <pre><code>helm upgrade --install btrix ... --set-file email.templates.invite=./invite.html\n</code></pre> <p>The list of available templates (and their default content) is available here</p> <p>The format of the template file is, for HTML emails:</p> <pre><code>Subject\n~~~\nHTML Content\n~~~\nText Content\n</code></pre> <p>or, for plain text emails:</p> <pre><code>Subject\n~~~\nText\n</code></pre> <p>The <code>~~~</code> is used to separate the sections. If only two sections are provided, the email template is treated as plain text, if three, an HTML email with plain text fallback is sent.</p>"},{"location":"deploy/customization/#signing-wacz-files","title":"Signing WACZ files","text":"<p>Browsertrix has the ability to cryptographically sign WACZ files with Authsign. The <code>signer</code> setting can be used to enable this feature and configure Authsign.</p>"},{"location":"deploy/local/","title":"Local Deployment","text":"<p>To try out the latest release of Browsertrix on your local machine, you'll first need to have a working Kubernetes cluster.</p>"},{"location":"deploy/local/#installing-kubernetes","title":"Installing Kubernetes","text":"<p>Before running Browsertrix, you'll need to set up a running Kubernetes cluster.</p> <p>Today, there are numerous ways to deploy Kubernetes fairly easily, and we recommend trying one of the single-node options, which include Docker Desktop, microk8s, minikube, and k3s.</p> <p>The instructions below assume a local package manager for your platform (eg. <code>brew</code> for macOS, <code>choco</code> for Windows, etc...) is already installed.</p> <p>Cloning the repository at https://github.com/webrecorder/browsertrix is only needed to access additional configuration files.</p> <p>Here are some environment specific instructions for setting up a local cluster from different Kubernetes vendors:</p> Docker Desktop (recommended for macOS and Windows) <p>For macOS and Windows, we recommend testing out Browsertrix using Kubernetes support in Docker Desktop as that will be one of the simplest options.</p> <ol> <li> <p>Install Docker Desktop if not already installed.</p> </li> <li> <p>Under Settings &gt; Kubernetes, ensure Enable Kubernetes is checked.</p> </li> <li> <p>Restart Docker Desktop if asked, and wait for it to fully restart.</p> </li> <li> <p>Install Helm, with <code>brew install helm</code> (macOS) or <code>choco install kubernetes-helm</code> (Windows) or following some of the other install options</p> </li> </ol> MicroK8S (recommended for Ubuntu) <p>For Ubuntu and other Linux distros, we recommend using MicroK8S for both local deployment and production.</p> <ol> <li> <p>Install MicroK8s, by running <code>sudo snap install microk8s --classic</code> see more detailed instructions here or alternate installation instructions here.</p> </li> <li> <p>Install the following addons: <code>microk8s enable dns hostpath-storage registry helm3</code>. (For production, also add <code>ingress cert-manager</code> to the list of addons)</p> </li> <li> <p>Wait for add-ons to finish installing with <code>microk8s status --wait-ready</code></p> </li> </ol> <p>Note: microk8s comes with its own version helm, so you don't need to install it separately. Replace <code>helm</code> with <code>microk8s helm3</code> in the subsequent instructions below.</p> Minikube (Windows, macOS, or Linux) <ol> <li> <p>Install Minikube following installation instructions, eg. <code>brew install minikube</code>.    Note that Minikube also requires Docker or another container management system to be installed as well.</p> </li> <li> <p>Install Helm, with <code>!sh brew install helm</code> (macOS) or <code>choco install kubernetes-helm</code> (Windows) or following some of the other install options</p> </li> </ol> K3S (recommended for non-Ubuntu Linux) <ol> <li> <p>Install K3s as per the instructions</p> </li> <li> <p>Install Helm, with <code>brew install helm</code> (macOS) or <code>choco install kubernetes-helm</code> (Windows) or following some of the other install options</p> </li> <li> <p>Set <code>KUBECONFIG</code> to point to the config for K3S: <code>export KUBECONFIG=/etc/rancher/k3s/k3s.yaml</code> to ensure Helm will use the correct version.</p> </li> </ol>"},{"location":"deploy/local/#launching-browsertrix-with-helm-repository","title":"Launching Browsertrix with Helm Repository","text":"<p>Once you have a running Kubernetes cluster with Helm 3 installed using one of the options, you can add the Browsertrix Helm Repository:</p> <pre><code>helm repo add browsertrix https://docs.browsertrix.com/helm-repo/\n</code></pre> <p>and then install the latest version of the Browsertrix helm chart with:</p> <pre><code>helm upgrade --install btrix browsertrix/browsertrix\n</code></pre> <p>You can optionally specify a specific version with the <code>--version</code> flag:</p> <p></p> <pre><code>helm upgrade --install btrix browsertrix/browsertrix --version VERSION\n</code></pre> <p>The versions correspond to the available Release Tags</p>"},{"location":"deploy/local/#installing-from-github-release-directly","title":"Installing from GitHub Release Directly","text":"<p>Alternatively, you can also use Helm to install a specific version of Browsertrix directly from the latest GitHub release, if you don't wish to add the Helm repository</p> <p></p> <pre><code>helm upgrade --install btrix \\\nhttps://github.com/webrecorder/browsertrix/releases/download/VERSION/browsertrix-VERSION.tgz\n</code></pre> <p>However, the Helm repository option is recommended as it makes upgrading to the latest version easier.</p> MicroK8S <p>If using microk8s, the commands will be:</p> <pre><code>microk8s helm3 repo add browsertrix https://docs.browsertrix.com/helm-repo/\nmicrok8s helm3 upgrade --install btrix browsertrix/browsertrix\n</code></pre> <p>for the Helm repo option or, for a direct install:</p> <p></p> <pre><code>microk8s helm3 upgrade --install btrix \\\nhttps://github.com/webrecorder/browsertrix/releases/download/VERSION/browsertrix-VERSION.tgz\n</code></pre> <p>Note: Subsequent commands will also use <code>microk8s helm3</code> instead of <code>helm</code>.</p> <p>The default setup includes the full Browsertrix system, with frontend, backend api, db (via MongoDB), and storage (via Minio)</p> <p>An admin user with name <code>admin@example.com</code> and password <code>PASSW0RD!</code> will be automatically created.</p> <p>With Helm, additional YAML files can be added to further override previous settings.</p> <p>Some possible settings can be changed are found in chart/examples/local-config.yaml.</p> <p>For example, to change the default superadmin, uncomment the <code>superadmin</code> block in <code>local-config.yaml</code>, and then change the username (<code>admin@example.com</code>) and password (<code>PASSW0RD!</code>) to different values. (The admin username and password will be updated with each deployment). To change the local port, change <code>local_service_port</code> setting.</p> <p>You can then redeploy with these additional settings by running:</p> <p></p> <pre><code>helm upgrade --install btrix https://github.com/webrecorder/browsertrix/releases/download/VERSION/browsertrix-VERSION.tgz \\\n-f ./chart/examples/local-config.yaml\n</code></pre> <p>The above examples assumes running from a cloned Browsertrix repo, however the config file can be saved anywhere and specified with <code>-f &lt;extra-config.yaml&gt;</code>.</p>"},{"location":"deploy/local/#waiting-for-cluster-to-start","title":"Waiting for Cluster to Start","text":"<p>After running the helm command, you should see something like:</p> <pre><code>Release \"btrix\" does not exist. Installing it now.\nNAME: btrix\nLAST DEPLOYED: &lt;time&gt;\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre> <p>After that, especially on first run, it may take a few minutes for the Browsertrix cluster to start, as all images need to be downloaded locally.</p> <p>You can try running the following command to wait for all pods to be initialized: </p> <pre><code>kubectl wait --for=condition=ready pod --all --timeout=300s\n</code></pre> <p>The command will exit when all pods have been loaded, or if there is an error and it times out.</p> <p>If the command succeeds, you should be able to access Browsertrix by loading: http://localhost:30870/ in your browser.</p> Minikube (on macOS) <p>When using Minikube on a macOS, the port will not be 30870. Instead, Minikube opens a tunnel to a random port, obtained by running <code>minikube service browsertrix-cloud-frontend --url</code> in a separate terminal. Use the provided URL (in the format <code>http://127.0.0.1:&lt;TUNNEL_PORT&gt;</code>) instead.</p>"},{"location":"deploy/local/#debugging-pod-issues","title":"Debugging Pod Issues","text":"<p>If this command fails, you can also run <code>kubectl get pods</code> to see the status of each of the pods.</p> <p>There should be 4 pods listed: backend, frontend, minio, and mongodb. If any one is not ready for a while, something may be wrong.</p> <p>To get more details about why a pod has not started, run <code>kubectl describe &lt;podname&gt;</code> and see the latest status at the bottom.</p> <p>Often, the error may be obvious, such as failed to pull an image.</p> <p>If the pod is running, or previously ran, you can also get the logs from the container by running <code>kubectl logs &lt;podname&gt;</code></p> <p>The outputs of these commands are helpful when reporting an issue on GitHub</p>"},{"location":"deploy/local/#updating-the-cluster","title":"Updating the Cluster","text":"<p>To update the cluster, for example to update to new version <code>NEWVERSION</code>, re-run the same command again, which will pull the latest images. In this way, you can upgrade to the latest release of Browsertrix. The upgrade will preserve the database and current archives.</p> <pre><code>helm upgrade --install btrix https://github.com/webrecorder/browsertrix/releases/download/NEWVERSION/browsertrix-NEWVERSION.tgz\n</code></pre>"},{"location":"deploy/local/#uninstalling","title":"Uninstalling","text":"<p>To uninstall, run <code>helm uninstall btrix</code>.</p> <p>By default, the database + storage volumes are not automatically deleted, run <code>helm upgrade ...</code> again to restart the cluster in its current state.</p> <p>If you are upgrading from a previous version, and run into issues with <code>helm upgrade ...</code>, we recommend uninstalling and then re-running upgrade.</p>"},{"location":"deploy/local/#deleting-all-data","title":"Deleting all Data","text":"<p>To fully delete all persistent data (db + archives) created in the cluster, run <code>kubectl delete pvc --all</code> after uninstalling.</p>"},{"location":"deploy/local/#deploying-for-local-development","title":"Deploying for Local Development","text":"<p>These instructions are intended for deploying the cluster from the latest releases published on GitHub. See setting up cluster for local development for additional customizations related to developing Browsertrix and deploying from local images.</p>"},{"location":"deploy/remote/","title":"Remote: Self-Hosted and Cloud","text":"<p>For remote and hosted deployments (both on a single machine or in the cloud), the only requirement is to have a designed domain and (strongly recommended, but not required) second domain for signing web archives. </p> <p>We are also experimenting with Ansible playbooks for cloud deployment setups.</p> <p>The production deployments also allow using an external mongodb server, and/or external S3-compatible storage instead of the bundled minio.</p>"},{"location":"deploy/remote/#single-machine-deployment-with-microk8s","title":"Single Machine Deployment with MicroK8S","text":"<p>For a single-machine remote deployment, we recommend using MicroK8s.</p> <ol> <li> <p>Install MicroK8S, as suggested in the local deployment guide and ensure the <code>ingress</code> and <code>cert-manager</code> addons are also enabled.</p> </li> <li> <p>Copy <code>cp ./chart/examples/microk8s-hosted.yaml ./chart/my-config.yaml</code> to make local changes.</p> </li> <li> <p>Set the <code>ingress.host</code>, <code>ingress.cert_email</code> and <code>signing.host</code> fields in <code>./chart/my-config.yaml</code> to your host and domain</p> </li> <li> <p>Set the super-admin username and password, and mongodb username and password in <code>./chart/my-config.yaml</code></p> </li> <li> <p>Run with:</p> </li> </ol> <pre><code>helm upgrade --install -f ./chart/values.yaml -f ./chart/my-config.yaml btrix ./chart/\n</code></pre>"},{"location":"deploy/remote/#single-machine-deployment-with-k3s","title":"Single Machine Deployment with k3s","text":"<p>Another option for a single-machine remote deployment is k3s</p> <ol> <li> <p>Install K3s, as suggested in the local deployment guide. Make sure to disable traefik which can be done by adding <code>--no-deploy traefik</code> to the <code>systemd</code> unit when installing k3s</p> </li> <li> <p>Install <code>nginx-ingress</code> with: <code>helm upgrade --install nginx ingress-nginx/ingress-nginx -n ingress-nginx --create-namespace</code></p> </li> <li>Install <code>cert-manager</code>. We recommend installing <code>cert-manager</code> through Jetpack, like so: </li> </ol> <pre><code>helm repo add jetstack https://charts.jetstack.io\nhelm repo update\n\nhelm repo update jetstack\nhelm upgrade --install \\\n  cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --version v1.12.0 \\\n  --set installCRDs=true\n</code></pre> <ol> <li> <p>Copy <code>cp ./chart/examples/k3s-hosted.yaml ./chart/my-config.yaml</code> to make local changes.</p> </li> <li> <p>Set the <code>ingress.host</code>, <code>ingress.cert_email</code> and <code>signing.host</code> fields in <code>./chart/my-config.yaml</code> to your host and domain</p> </li> <li> <p>Set the super-admin username and password, and mongodb username and password in <code>./chart/my-config.yaml</code></p> </li> <li> <p>Run with:</p> </li> </ol> <pre><code>helm upgrade --install -f ./chart/values.yaml -f ./chart/my-config.yaml btrix ./chart/\n</code></pre>"},{"location":"deploy/remote/#using-custom-storage","title":"Using Custom Storage","text":"<p>If you would like to use existing external storage, such an existing S3-compatible storage, also set the default storage, for example:</p> <pre><code>minio_local: false\n\nstorages:\n  - name: \"default\"\n    access_key: &lt;access key&gt;\n    secret_key: &lt;secret key&gt;\n\n    endpoint_url: \"https://s3.&lt;region&gt;.amazonaws.com/bucket/path/\"\n</code></pre> <p>Note that this setup is not limited to Amazon S3, but should work with any S3-compatible storage service.</p>"},{"location":"deploy/remote/#using-custom-mongodb","title":"Using Custom MongoDB","text":"<p>If you would like to use an externally hosted MongoDB, you can add the following config to point to a custom MongoDB instance.</p> <p>The <code>db_url</code> should follow the MongoDB Connection String Format which should include the username and password of the remote instance.</p> <pre><code>mongo_local: false\n\nmongo_auth:\n  db_url: mongodb+srv://...\n</code></pre>"},{"location":"deploy/remote/#cloud-deployment","title":"Cloud Deployment","text":"<p>There are also many ways to deploy Browsertrix on various cloud providers.</p> <p>To simplify this process, we are working on Ansible playbooks for setting up Browsertrix on commonly used infrastructure.</p>"},{"location":"deploy/remote/#ansible-deployment","title":"Ansible Deployment","text":"<p>Ansible makes the initial setup and configuration of your Browsertrix instance automated and repeatable. </p> <p>To use, you will need to install Ansible on your control computer and then you can use these to deploy to Browsertrix on remote and cloud environments.</p> <p>Currently, we provide playbooks for the following tested environments:</p> <ul> <li>DigitalOcean</li> <li>Microk8s</li> <li>k3s</li> </ul>"},{"location":"deploy/ansible/digitalocean/","title":"DigitalOcean","text":"<p>Playbook Path: ansible/playbooks/install_microk8s.yml</p> <p>This playbook provides an easy way to install Browsertrix on DigitalOcean. It automatically sets up Browsertrix with LetsEncrypt certificates.</p>"},{"location":"deploy/ansible/digitalocean/#requirements","title":"Requirements","text":"<p>To run this ansible playbook, you need to:</p> <ul> <li>Have a DigitalOcean Account where this will run.</li> <li>Create a DigitalOcean API Key which will need to be set in your terminal sessions environment variables <code>export DO_API_TOKEN</code> </li> <li><code>doctl</code> command line client configured (run <code>doctl auth init</code>)</li> <li>Create a DigitalOcean Spaces API Key which will also need to be set in your terminal sessions environment variables, which should be set as <code>DO_AWS_ACCESS_KEY</code> and <code>DO_AWS_SECRET_KEY</code></li> <li>Configure a DNS A Record and CNAME record.</li> <li>Have a working Python and pip configuration through your OS Package Manager</li> </ul>"},{"location":"deploy/ansible/digitalocean/#install","title":"Install","text":"<ol> <li> <p>Clone the repo: <pre><code>git clone https://github.com/webrecorder/browsertrix.git\ncd browsertrix\n</code></pre></p> </li> <li> <p>Install the Dependencies through pipenv <pre><code>cd ansible\npip install pipenv\npipenv install\npipenv shell\n</code></pre></p> </li> <li> <p>Look at the configuration options and modify them or pass them as extra variables as shown below. If you haven't configured <code>kubectl</code>, please enable the <code>configure_kube</code> option </p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook do_setup.yml -e project_name=\"your-project\" -e superuser_email=\"you@yourdomain.com\" -e domain=\"yourdomain.com\"\n</code></pre></p> </li> </ol> <p>You may optionally configure these command line parameters through the group_vars file</p>"},{"location":"deploy/ansible/digitalocean/#upgrading","title":"Upgrading","text":"<ol> <li> <p>Run <code>git pull</code></p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook do_setup.yml -e project_name=\"your-project\" -e superuser_email=\"you@yourdomain.com\" -e domain=\"yourdomain.com\" -t helm_upgrade\n</code></pre></p> </li> </ol>"},{"location":"deploy/ansible/digitalocean/#uninstall","title":"Uninstall","text":"<p>You can tear down your deployment through ansible as well. By default ansible will dump all the databases into your DO space. You can configure an option to disable this feature. </p> <pre><code>ansible-playbook playbooks/do_teardown.yml -e project_name=\"your-project\" -e superuser_email=\"you@yourdomain.com\" -e domain=\"yourdomain.com\"\n</code></pre>"},{"location":"deploy/ansible/k3s/","title":"K3S","text":"<p>Playbook Path: ansible/playbooks/install_k3s.yml</p> <p>This playbook provides an easy way to install Browsertrix on a Linux box (tested on Rocky Linux 9). It automatically sets up Browsertrix with Let's Encrypt certificates.</p>"},{"location":"deploy/ansible/k3s/#requirements","title":"Requirements","text":"<p>To run this ansible playbook, you need to:</p> <ul> <li>Have a server / VPS where Browsertrix will run.</li> <li>Configure a DNS A Record to point at your server's IP address.</li> <li>Make sure SSH is working, with a sudo user: <code>ssh your-user@your-domain</code></li> <li> <p>Install Ansible on your local machine (the control machine).</p> </li> <li> <p>Clone the repo: <pre><code>git clone https://github.com/webrecorder/browsertrix.git\ncd browsertrix\n</code></pre></p> </li> <li> <p>Optional: Create a copy of the [inventory directory] and name it what you like (alternatively edit the sample files in place) <pre><code>cp -r ansible/inventory/sample-k3s ansible/inventory/my-deployment\n</code></pre></p> </li> <li> <p>Look at the configuration options and modify them to match your setup </p> </li> <li> <p>Change the hosts IP address in your just created inventory</p> </li> <li> <p>You may need to make modifications to the playbook itself based on your configuration. The playbook lists sections that can be removed or changed based on whether you'd like to install a multi-node or single-node k3s installation for your Browsertrix deployment. By default the playbook assumes you'll run in a single-node environment deploying directly to <code>localhost</code></p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook -i inventory/my-deployment/hosts.ini install_k3s.yml\n</code></pre></p> </li> </ul>"},{"location":"deploy/ansible/k3s/#upgrading","title":"Upgrading","text":"<ol> <li> <p>Run <code>git pull</code></p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook -i inventory/hosts install_k3s.yml -t helm_upgrade\n</code></pre></p> </li> </ol>"},{"location":"deploy/ansible/microk8s/","title":"Microk8s","text":"<p>Playbook Path: ansible/playbooks/install_microk8s.yml</p> <p>This playbook provides an easy way to install Browsertrix on Ubuntu (tested on Jammy Jellyfish) and RedHat 9 (tested on Rocky Linux 9). It automatically sets up Browsertrix with Letsencrypt certificates.</p>"},{"location":"deploy/ansible/microk8s/#requirements","title":"Requirements","text":"<p>To run this ansible playbook, you need to:</p> <ul> <li>Have a server / VPS where browsertrix will run.</li> <li>Configure a DNS A Record to point at your server's IP address.</li> <li>Make sure you can ssh to it, with a sudo user: ssh @ <li>Install Ansible on your local machine (the control machine).</li> <p>Note</p> <p>Ansible requires an SSH key with no password. You cannot use a passphrase.     Sudo must similarly be available without a passphrase for ansible to work</p> Info <pre><code>You will need to install `acl` on the target Ansible machine to set permissions: \n`sudo apt-get install acl`\n</code></pre>"},{"location":"deploy/ansible/microk8s/#install","title":"Install","text":"<ol> <li> <p>Clone the repo: <pre><code>git clone https://github.com/webrecorder/browsertrix.git\ncd browsertrix/ansible\n</code></pre></p> </li> <li> <p>Look at the configuration options and modify them or pass them as extra variables as shown below. </p> </li> <li> <p>Add your IP address above to a new file called [inventory/hosts]</p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook -i inventory/hosts install_microk8s.yml -e host_ip=\"1.2.3.4\" -e domain_name=\"yourdomain.com\" -e your_user=\"your_vps_admin_user\"\n</code></pre></p> </li> </ol>"},{"location":"deploy/ansible/microk8s/#upgrading","title":"Upgrading","text":"<ol> <li> <p>Run <code>git pull</code></p> </li> <li> <p>Run the playbook: <pre><code>ansible-playbook -i inventory/hosts install_microk8s.yml -e host_ip=\"1.2.3.4\" -e domain_name=\"yourdomain.com\" -t helm_upgrade\n</code></pre></p> </li> </ol>"},{"location":"develop/","title":"Developing Browsertrix","text":"<p>Browsertrix consists of a Python-based backend and TypeScript-based frontend.</p> <p>To develop Browsertrix, the system must first be deployed locally in a Kubernetes cluster.</p> <p>The deployment can then be further customized for local development.</p>"},{"location":"develop/#backend","title":"Backend","text":"<p>The backend is an API-only system, using the FastAPI framework. Latest API docs can be viewed in the browser by adding <code>/api/redoc</code> to the URL of a running cluster (ex: <code>http://localhost:30870/api/redoc</code> when running locally on port <code>30870</code>.)</p> <p>At this time, the backend must be deployed in the Kubernetes cluster.</p>"},{"location":"develop/#frontend","title":"Frontend","text":"<p>The frontend UI is implemented in TypeScript, using the Lit framework and Shoelace component library.</p> <p>The static build of the frontend is bundled with nginx, but the frontend can be deployed locally in dev mode against an existing backend.</p> <p>See Developing the Frontend UI for more details.</p>"},{"location":"develop/docs/","title":"Writing Documentation","text":"<p>Our documentation is built with Material for MkDocs and configured via <code>mkdocs.yml</code> in the project root.</p> <p>The docs can be found in the <code>./docs</code> subdirectory.</p> <p>To build the docs locally, install Material for MkDocs with pip:</p> <pre><code>pip install mkdocs-material\n</code></pre> <p>In the project root directory run <code>mkdocs serve</code> to run a local version of the documentation site.</p> <p>The docs hosted on docs.browsertrix.com are created from the main branch of https://github.com/webrecorder/browsertrix</p>"},{"location":"develop/docs/#adding-new-pages","title":"Adding New Pages","text":"<ol> <li>Create a Markdown file in the directory of choice</li> <li>Add the newly created Markdown file to the <code>nav</code> value under the subsection as defined by the file's location in <code>mkdocs.yml</code>.</li> </ol>"},{"location":"develop/docs/#adding-icons","title":"Adding Icons","text":"<p>We typically use the Bootstrap icon set with our projects. This set is quite expansive, and we don't add the entire set into our docs folder as most icons go unused. If you wish to use an icon when writing documentation to refer to an icon present in part of the app, you may have to download the SVG file and add it to the repo.</p> <p>Icons are placed in the <code>docs/overrides/.icons/iconsetname/icon-name.svg</code> directory, and can be added in markdown files as <code>:iconsetname-icon-name:</code> accordingly. After adding icons to the folder, MKDocs must be restarted. For more information, see the Material for MKDocs page on Changing the logo and icons.</p>"},{"location":"develop/docs/#docs-style-guide","title":"Docs Style Guide","text":""},{"location":"develop/docs/#american-english","title":"American English","text":"<p>Webrecorder is a global team but we use American English when writing documentation and in-app copy. Some basic rules to follow are:</p> <ol> <li>Swap the <code>s</code> for a <code>z</code> in words like categorize and pluralize.</li> <li>Remove the <code>u</code> from words like color and honor.</li> <li>Swap <code>tre</code> for <code>ter</code> in words like center.</li> <li>Numbers should be formatted with commas for separation of values, using periods to denote decimals (e.g: 3,153.89, not 3 153,89).</li> </ol>"},{"location":"develop/docs/#oxford-commas","title":"Oxford Commas","text":"<p>In a list of three or more items, the list item proceeding the word \"and\" should have a comma placed after it clarifying that the final item in the list is not a part of the previous item.</p>"},{"location":"develop/docs/#example","title":"Example","text":"Use Don't use One, two, three, and four. One, two, three and four. Charles, Ada, and Alan. Charles, Ada and Alan."},{"location":"develop/docs/#capitalization-of-concepts-and-tools","title":"Capitalization of Concepts and Tools","text":"<p>Webrecorder has a number of common nouns that we use in our products. Examples include: archived items, crawl workflows, browser profiles, collections, and organizations. Because these are concepts and not specific instances of each concept, do not capitalize them unless they are at the start of a sentence.</p>"},{"location":"develop/docs/#example_1","title":"Example","text":"<p>When starting a sentence:</p> <p>Archived items consist of one or more...</p> <p>In the middle of a sentence:</p> <p>...they are omitted from the archived items list page...</p> <p>Webrecorder's software packages are all proper nouns and should always be capitalized.  Examples include: Browsertrix, ReplayWeb.page, ArchiveWeb.Page, and PYWB. Specific pages such as the Archived Items page should also be capitalized as they are not referencing the concept of archived items and are instead referencing the page in question that happens to share the same name.</p>"},{"location":"develop/docs/#be-concise-avoid-you-statements","title":"Be Concise, Avoid \"You Statements\"","text":"<p>Generally, people don't want to have to read documentation. When writing, try to explain concepts simply and with clear objective language. Do not use \"we\" to refer to communication between the author and the reader, use \"we\" to refer to Webrecorder. \"You can\" or \"you may\" can be used, but preferably when giving supplemental advice and generally not when providing instructions that should be followed to achieve a successful outcome. Otherwise, avoid spending time referring to the reader, instead tell them what they should know.</p>"},{"location":"develop/docs/#example_2","title":"Example","text":"<p>If you want to do x, you can click on y.</p> <p>Can often be shortened to:</p> <p>To do x, click on y.</p>"},{"location":"develop/docs/#acronyms","title":"Acronyms","text":"<p>Avoid using acronyms when reuse is not frequent enough to warrant space savings. When acronyms must be used, spell the full phrase first and include the acronym in parentheses <code>()</code> the first time it is used in each document. This can be omitted for extremely common acronyms such as \"URL\" or \"HTTP\".</p>"},{"location":"develop/docs/#example_3","title":"Example","text":"<p>When running in a Virtual Machine (VM), use the....</p>"},{"location":"develop/docs/#headings","title":"Headings","text":"<p>All headings should be set in title case.</p>"},{"location":"develop/docs/#example_4","title":"Example","text":"<p>Indiana Jones and the Raiders of the Lost Ark</p>"},{"location":"develop/docs/#referencing-features-and-their-options","title":"Referencing Features and Their Options","text":"<p>Controls with multiple options should have their options referenced as <code>in-line code blocks</code>.</p> <p>Setting names referenced outside of a heading should be Title Cased and set in italics.</p> <p>Actions with text (buttons in the app) should also be Title Cased and set in italics.</p>"},{"location":"develop/docs/#example_5","title":"Example","text":"<p>Sets the day of the week for which crawls scheduled with a <code>Weekly</code> Frequency will run.</p>"},{"location":"develop/docs/#manual-word-wrapping","title":"Manual Word Wrapping","text":"<p>Do not manually wrap words by adding newlines when writing documentation.</p>"},{"location":"develop/docs/#code-block-syntax-highlighting","title":"Code Block Syntax Highlighting","text":"<p>Tag the language to be used for syntax highlighting.</p>"},{"location":"develop/docs/#example_6","title":"Example","text":"<pre><code> ```markdown\n example markdown code block text\n ```\n</code></pre> <p>For in-line code blocks, syntax highlighting should be added for all code-related usage by adding <code>#!language</code> to the start of all in-line code blocks. This is not required for paths or simply highlighting important text using in-line code blocks.</p>"},{"location":"develop/docs/#example_7","title":"Example","text":"<pre><code> `#!python range()`\n</code></pre> <p>Renders to: <code>range()</code></p>"},{"location":"develop/docs/#paid-features","title":"Paid features","text":"<p><code>Paid Feature</code></p> <p>Some features of Browsertrix only pertain to those paying for the software on a hosted plan. Denote these with the following:</p> <pre><code>`Paid Feature`{ .badge-green }\n</code></pre>"},{"location":"develop/docs/#admonitions","title":"Admonitions","text":"<p>We use Admonitions in their collapsed state to offer additional context or tips that aren't relevant to all users reading the section. We use standard un-collapsible ones when we need to call attention to a specific point.</p> <p>There are a lot of different options provided by Material for MkDocs \u2014 So many in fact that we try to pair down their usage into the following categories.</p> Note <p>The default call-out, used to highlight something if there isn't a more relevant one \u2014 should generally be expanded by default but can be collapsible by the user if the note is long.</p> <p>Tip: May have a title stating the tip or best practice</p> <p>Used to highlight a point that is useful for everyone to understand about the documented subject \u2014 should be expanded and kept brief.</p> Info: Must have a title describing the context under which this information is useful <p>Used to deliver context-based content such as things that are dependant on operating system or environment \u2014 should be collapsed by default.</p> Example: Must have a title describing the content <p>Used to deliver additional information about a feature that could be useful in a specific circumstance or that might not otherwise be considered \u2014 should be collapsed by default.</p> Question: Must have a title phrased in the form of a question <p>Used to answer frequently asked questions about the documented subject \u2014 should be collapsed by default.</p> <p>Warning: Must have a title stating the warning</p> <p>Used to deliver important information \u2014 should always be expanded.</p> <p>Danger: Must have a title stating the warning</p> <p>Used to deliver information about serious unrecoverable actions such as deleting large amounts of data or resetting things \u2014 should always be expanded.</p>"},{"location":"develop/frontend-dev/","title":"Developing the Frontend UI","text":"<p>This guide explains how to run the Browsertrix frontend development server with Yarn.</p> <p>Instead of rebuilding the entire frontend image to view your UI changes, you can use the included local development server to access the frontend from your browser. This setup is ideal for rapid UI development that does not rely on any backend changes.</p>"},{"location":"develop/frontend-dev/#requirements","title":"Requirements","text":""},{"location":"develop/frontend-dev/#1-browsertrix-api-backend-already-in-a-kubernetes-cluster","title":"1. Browsertrix API backend already in a Kubernetes cluster","text":"<p>The frontend development server requires an existing backend that has been deployed locally or is in production. See Deploying Browsertrix.</p>"},{"location":"develop/frontend-dev/#2-nodejs-18-and-yarn-1","title":"2. Node.js \u226518 and Yarn 1","text":"<p>To check if you already have Node.js installed, run the following command in your command line terminal:</p> <pre><code>node --version\n</code></pre> <p>You should see a version number like <code>v18.12.1</code>. If you see a command line error instead of a version number, install Node.js before continuing.</p> What if my other project requires a different version of Node.js? <p>You can use Node Version Manager to install multiple Node.js versions and switch versions between projects.</p> <p>To check your Yarn installation:</p> <pre><code>yarn --version\n</code></pre> <p>You should see a version number like <code>1.22.19</code>. If you do not, install or upgrade Yarn.</p>"},{"location":"develop/frontend-dev/#quickstart","title":"Quickstart","text":"<p>From the command line, change your current working directory to <code>/frontend</code>:</p> <pre><code>cd frontend\n</code></pre> <p>Note</p> <p>From this point on, all commands in this guide should be run from the <code>frontend</code> directory.</p> <p>Install UI dependencies:</p> <pre><code>yarn install\n</code></pre> <p>Copy environment variables from the sample file:</p> <pre><code>cp sample.env.local .env.local\n</code></pre> <p>Update <code>API_BASE_URL</code> in <code>.env.local</code> to point to your backend API host. For example:</p> <pre><code>API_BASE_URL=http://dev.example.com\n</code></pre> <p>Note</p> <p>This setup assumes that your API endpoints are available under <code>/api</code>, which is the default configuration for the Browsertrix backend.</p> <p>If connecting to a local deployment cluster, set <code>API_BASE_URL</code> to:</p> <pre><code>API_BASE_URL=http://localhost:30870\n</code></pre> Port when using Minikube (on macOS) <p>When using Minikube on macOS, the port will not be 30870. Instead, Minikube opens a tunnel to a random port, obtained by running <code>minikube service browsertrix-cloud-frontend --url</code> in a separate terminal.</p> <p>Set API_BASE_URL to provided URL instead, eg. <code>API_BASE_URL=http://127.0.0.1:&lt;TUNNEL_PORT&gt;</code></p> <p>Start the frontend development server:</p> <pre><code>yarn start\n</code></pre> <p>This will open <code>localhost:9870</code> in a new tab in your default browser.</p> <p>Saving changes to files in <code>src</code> will automatically reload your browser window with the latest UI updates.</p> <p>To stop the development server type Ctrl+C into your command line terminal.</p>"},{"location":"develop/frontend-dev/#scripts","title":"Scripts","text":"<code>yarn &lt;name&gt;</code> <code>start</code> runs app in development server, reloading on file changes <code>test</code> runs tests in chromium with playwright <code>build-dev</code> bundles app and outputs it in <code>dist</code> directory <code>build</code> bundles app, optimized for production, and outputs it to <code>dist</code> <code>lint</code> find and fix auto-fixable javascript errors <code>format</code> formats js, html, and css files <code>localize:extract</code> generate XLIFF file to be translated <code>localize:build</code> output a localized version of strings/templates"},{"location":"develop/frontend-dev/#testing","title":"Testing","text":"<p>Tests assertions are written in Chai.</p> <p>To watch for file changes while running tests:</p> <pre><code>yarn test --watch\n</code></pre> <p>To run tests in multiple browsers:</p> <pre><code>yarn test --browsers chromium firefox webkit\n</code></pre>"},{"location":"develop/frontend-dev/#localization","title":"Localization","text":"<p>Wrap text or templates in the <code>msg</code> helper to make them localizable:</p> <pre><code>// import from @lit/localize:\nimport { msg } from \"@lit/localize\";\n\n// later, in the render function:\nrender() {\n  return html`\n    &lt;button&gt;\n      ${msg(\"Click me\")}\n    &lt;/button&gt;\n  `\n}\n</code></pre> <p>Entire templates can be wrapped as well:</p> <pre><code>render() {\n  return msg(html`\n    &lt;p&gt;Click the button&lt;/p&gt;\n    &lt;button&gt;Click me&lt;/button&gt;\n  `)\n}\n</code></pre> <p>See: https://lit.dev/docs/localization/overview/#message-types</p> <p>To add new languages:</p> <ol> <li>Add BCP 47 language tag to <code>targetLocales</code> in <code>lit-localize.json</code></li> <li>Run <code>yarn localize:extract</code> to generate new .xlf file in <code>/xliff</code></li> <li>Provide .xlf file to translation team</li> <li>Replace .xlf file once translated</li> <li>Run <code>yarn localize:build</code> bring translation into <code>src</code></li> </ol> <p>See: https://lit.dev/docs/localization/overview/#extracting-messages</p>"},{"location":"develop/local-dev-setup/","title":"Setup for Local Development","text":""},{"location":"develop/local-dev-setup/#installation","title":"Installation","text":"<p>First, see our Local Deployment guide for instructions on how to install the latest release with Kubernetes with Helm 3.</p>"},{"location":"develop/local-dev-setup/#local-dev-configuration","title":"Local Dev Configuration","text":"<p>The local deployment guide explains how to deploy Browsertrix with latest published images.</p> <p>However, if you are developing locally, you will need to use your local images instead.</p> <p>We recommend the following setup:</p> <ol> <li> <p>Copy the provided <code>./chart/examples/local-config.yaml</code> Helm configuration file to a separate file <code>local.yaml</code>, so that local changes to it will not be accidentally committed to git. </p> <p>From the root directory:</p> <pre><code>cp ./chart/examples/local-config.yaml ./chart/local.yaml\n</code></pre> </li> <li> <p>Uncomment <code>backend_image</code>, <code>frontend_image</code>, and pull policies in <code>./chart/local.yaml</code>, which will ensure the local images are used: <pre><code>backend_image: docker.io/webrecorder/browsertrix-backend:latest\nfrontend_image: docker.io/webrecorder/browsertrix-frontend:latest\nbackend_pull_policy: 'Never'\nfrontend_pull_policy: 'Never'\n</code></pre></p> MicroK8S <p>For microk8s, the pull policies actually need to be set to <code>IfNotPresent</code> instead of <code>Never</code>:</p> <pre><code>backend_pull_policy: 'IfNotPresent'\nfrontend_pull_policy: 'IfNotPresent'\n</code></pre> <p>This will ensure images are pulled from the MicroK8S registry (configured in next section).</p> </li> <li> <p>Build the local backend and frontend images. The exact process depends on the Kubernetes environment you've selected in your initial deployment. Environment specific build instructions are as follows:</p> Docker Desktop <p>Rebuild the local images by running <code>./scripts/build-backend.sh</code> and/or <code>./scripts/build-frontend.sh</code> scripts to build the images in the local Docker.</p> MicroK8S <p>MicroK8s uses its own container registry, running on port 32000.</p> <ol> <li> <p>Ensure the registry add-on is enabled by running <code>microk8s enable registry</code></p> </li> <li> <p>Set <code>export REGISTRY=localhost:32000/</code> and then run <code>./scripts/build-backend.sh</code> and/or <code>./scripts/build-frontend.sh</code> to rebuild the images into the MicroK8S registry.</p> </li> <li> <p>In <code>./chart/local.yaml</code>, also uncomment the following lines to use the local images: <pre><code>backend_image: \"localhost:32000/webrecorder/browsertrix-backend:latest\"\nfrontend_image: \"localhost:32000/webrecorder/browsertrix-frontend:latest\"\n</code></pre></p> </li> </ol> Minikube <p>Minikube comes with its own image builder to update the images used in Minikube.</p> <p>To build the backend image, run:</p> <pre><code>minikube image build -t webrecorder/browsertrix-backend:latest ./backend\n</code></pre> <p>To build a local frontend image, run:</p> <pre><code>minikube image build -t webrecorder/browsertrix-frontend:latest ./frontend\n</code></pre> K3S <p>K3S uses <code>containerd</code> by default. To use local images, they need to be imported after rebuilding.</p> <ol> <li> <p>Rebuild the images with Docker by running by running <code>./scripts/build-backend.sh</code> and/or <code>./scripts/build-frontend.sh</code> scripts. (Requires Docker to be installed as well).</p> </li> <li> <p>Serializer the images to .tar: <pre><code>docker save webrecorder/browsertrix-backend:latest &gt; ./backend.tar\ndocker save webrecorder/browsertrix-frontend:latest &gt; ./frontend.tar\n</code></pre></p> </li> <li> <p>Import images into k3s containerd: <pre><code>k3s ctr images import --base-name webrecorder/browsertrix-backend:latest ./backend.tar\nk3s ctr images import --base-name webrecorder/browsertrix-frontend:latest ./frontend.tar\n</code></pre></p> </li> </ol> </li> <li> <p>To change other options, uncomment them as needed in <code>./chart/local.yaml</code> or add additional overrides from <code>./chart/values.yaml</code>. </p> <p>For example, to set a superuser email to <code>my_super_user_email@example.com</code> and password to <code>MySecretPassword!</code>, uncomment that block and set: <pre><code>superuser:\n# set this to enable a superuser admin\nemail: my_super_user_email@example.com\n\n# optional: if not set, automatically generated\n# change or remove this\npassword: MySecretPassword!\n</code></pre></p> </li> <li> <p>Once the images have been built and config changes made in <code>./chart/local.yaml</code>, the cluster can be re-deployed by running: <pre><code>helm upgrade --install -f ./chart/values.yaml \\\n-f ./chart/local.yaml btrix ./chart/\n</code></pre></p> MicroK8S <p>If using microk8s, the commend will be:</p> <pre><code>microk8s helm3 upgrade --install -f ./chart/values.yaml -f ./chart/local.yaml btrix ./chart/\n</code></pre> </li> </ol> <p>Refer back to the Local Development guide for additional information on running and debugging your local cluster.</p>"},{"location":"develop/local-dev-setup/#update-the-images","title":"Update the Images","text":"<p>After making any changes to backend code (in <code>./backend</code>) or frontend code (in <code>./frontend</code>), you'll need to rebuild the images as specified above, before running <code>helm upgrade ...</code> to re-deploy.</p> <p>Changes to settings in <code>./chart/local.yaml</code> can be deployed with <code>helm upgrade ...</code> directly.</p>"},{"location":"develop/local-dev-setup/#deploying-frontend-only","title":"Deploying Frontend Only","text":"<p>If you are just making changes to the frontend, you can also deploy the frontend separately using a dev server for quicker iteration.</p>"},{"location":"user-guide/","title":"Browsertrix User Guide","text":"<p>Welcome to the Browsertrix User Guide. This page covers the basics of using Browsertrix, Webrecorder's high-fidelity web archiving system.</p>"},{"location":"user-guide/#getting-started","title":"Getting Started","text":"<p>To get started crawling with Browsertrix:</p> <ol> <li>Create an account and join an organization as described here.</li> <li>After being redirected to the organization's overview page, click the Create New button in the top right and select Crawl Workflow to begin configuring your first crawl!</li> <li>For a simple crawl, choose the Seeded Crawl option, and enter a page url in the Crawl Start URL field. By default, the crawler will archive all pages under the starting path.</li> <li>Next, click Review &amp; Save, and ensure the Run on Save option is selected. Then click Save Workflow.</li> <li>Wait a moment for the crawler to start and watch as it archives the website!</li> </ol> <p>After running your first crawl, check out the following to learn more about Browsertrix's features:</p> <ul> <li>A detailed list of crawl workflow setup options.</li> <li>Adding exclusions to limit your crawl's scope and evading crawler traps by editing exclusion rules while crawling.</li> <li>Best practices for crawling with browser profiles to capture content only available when logged in to a website.</li> <li>Managing archived items, including uploading previously archived content.</li> <li>Organizing and combining archived items with collections for sharing and export.</li> <li>If you're an admin: Inviting collaborators to your org.</li> </ul>"},{"location":"user-guide/#have-more-questions","title":"Have more questions?","text":"<p>While our aim is to create intuitive interfaces, sometimes the complexities of web archiving require a little more explanation. If there's something that you found especially confusing or frustrating please get in touch!</p>"},{"location":"user-guide/archived-items/","title":"Archived Items","text":"<p>Archived Items consist of one or more WACZ files created by a crawl workflow, or uploaded to Browsertrix. They can be individually replayed, or combined with other archived items in a collection. The Archived Items page lists all items in the organization.</p>"},{"location":"user-guide/archived-items/#uploading-web-archives","title":"Uploading Web Archives","text":"<p>WACZ files can be given metadata and uploaded to Browsertrix by pressing the Upload WACZ button on the archived items list page. Only one WACZ file can be uploaded at a time.</p>"},{"location":"user-guide/archived-items/#status","title":"Status","text":"<p>The status of an archived item depends on its type. Uploads will always have the status   Uploaded, crawls have four possible states:</p> Status Description  Complete The crawl completed according to the workflow's settings. Workflows with limits set may stop running before they capture every queued page, but the resulting archived item will still be marked as \"Complete\".  Stopped The crawl workflow was stopped gracefully by a user and data is saved.  Stopped: Reason A workflow limit (listed as the reason) was reached and data is saved.  Canceled The crawl workflow was canceled by a user, no data is saved.  Failed A serious error occurred while crawling, no data is saved. <p>Because   Canceled and   Failed crawls do not contain data, they are omitted from the archived items list page and cannot be added to a collection.</p>"},{"location":"user-guide/archived-items/#archived-item-details","title":"Archived Item Details","text":"<p>The archived item details page is composed of the following sections, though some are only available for crawls and not uploads.</p>"},{"location":"user-guide/archived-items/#overview","title":"Overview","text":"<p>The Overview tab displays the item's metadata and statistics associated with its creation process.</p> <p>Metadata can be edited by pressing the pencil icon at the top right of the metadata section to edit the item's description, tags, and collections it is associated with.</p>"},{"location":"user-guide/archived-items/#quality-assurance","title":"Quality Assurance","text":"<p>The Quality Assurance tab displays crawl quality information collected from analysis runs and user assessment of pages. This is where you can start new analysis runs, view quality metrics from older runs, and delete previous analysis runs. This tab is not available for uploaded archived items and not accessible for users with viewer permissions.</p> <p>The pages list provides a record of all pages within the archived item, as well as any ratings or notes given to the page during review. If analysis has been run, clicking on a page in the pages list will go to that page in the review interface.</p>"},{"location":"user-guide/archived-items/#crawl-analysis","title":"Crawl Analysis","text":"<p>Running crawl analysis will re-visit all pages within the archived item, comparing the data collected during analysis with the data collected during crawling. Crawl analysis runs with the same workflow limit settings used during crawling.</p> <p>Crawl analysis can be run multiple times, though results should only differ if the crawler version has been updated between runs. The analysis process is being constantly improved and future analysis runs should produce better results. Analysis run data can be downloaded or deleted from the Analysis Runs tab. While they are stored as WACZ files, analysis run WACZs only contain analysis data and may not open correctly or be useful in other programs that replay archived content.</p> <p>Once a crawl has been analyzed \u2014 either fully, or partially \u2014 it can be reviewed by pressing the Review Crawl button. For more on reviewing crawls and how to interpret analysis data, see: Crawl Review.</p> <p><code>Paid Feature</code></p> <p>Like running a crawl workflow, running crawl analysis also uses execution time. Crawls and crawl analysis share the same concurrent crawling limit, but crawl analysis runs will be paused in favor of new crawls if the concurrent crawling limit is reached.</p>"},{"location":"user-guide/archived-items/#replay","title":"Replay","text":"<p>The Replay tab displays the web content contained within the archived item.</p> <p>For more details on navigating web archives within ReplayWeb.page, see the ReplayWeb.page user documentation.</p>"},{"location":"user-guide/archived-items/#exporting-files","title":"Exporting Files","text":"<p>While crawling, Browsertrix will output one or more WACZ files \u2014 the crawler aims to output files in consistently sized chunks, and each crawler instance will output separate WACZ files.</p> <p>The Files tab lists the individually downloadable WACZ files that make up the archived item as well as their file sizes and backup status. To combine one or more archived items and download them all as a single WACZ file, add them to a collection and download the collection.</p>"},{"location":"user-guide/archived-items/#error-logs","title":"Error Logs","text":"<p>The Error Logs tab displays a list of errors encountered during crawling. Clicking an errors in the list will reveal additional information.</p> <p>All log entries with that were recorded in the creation of the Archived Item can be downloaded in JSONL format by pressing the Download Logs button.</p>"},{"location":"user-guide/archived-items/#crawl-settings","title":"Crawl Settings","text":"<p>The Crawl Settings tab displays the crawl workflow configuration options that were used to generate the resulting archived item. Many of these settings also apply when running crawl analysis.</p>"},{"location":"user-guide/browser-profiles/","title":"Browser Profiles","text":"<p>Browser profiles are saved instances of a web browsing session that can be reused to crawl websites as they were configured, with any cookies, saved login sessions, or browser settings. Using a pre-configured profile also means that content that can only be viewed by logged in users can be archived, without archiving the actual login credentials.</p> <p>Best practice: Create and use web archiving-specific accounts for crawling with browser profiles</p> <p>For the following reasons, we recommend creating dedicated accounts for archiving anything that is locked behind login credentials but otherwise public, especially on social media platforms.</p> <ul> <li> <p>While user names and passwords are not, the access tokens for logged in websites used in the browser profile creation process are stored by the server.</p> </li> <li> <p>Some websites may rate limit or lock accounts for reasons they deem to be suspicious, such as logging in from a new location or any crawling-related activity.</p> </li> <li> <p>While login information (username, password) is not archived, other data such as cookies, location, etc.. may be included in the resulting crawl (after all, personalized content is often the goal of sites that require credentials to view content).</p> </li> <li> <p>Due to nature of social media specifically, existing accounts may have personally identifiable information, even when accessing otherwise public content.</p> </li> </ul> <p>Of course, there are exceptions \u2014 such as when the goal is to archive personalized or private content accessible only from designated accounts. In these instances we recommend changing the account's password after crawling is complete.</p>"},{"location":"user-guide/browser-profiles/#creating-new-browser-profiles","title":"Creating New Browser Profiles","text":"<p>New browser profiles can be created on the Browser Profiles page by pressing the New Browser Profile button and providing a starting URL. </p> <p>Press the Finish Browsing button to save the browser profile with a Name and Description of what is logged in or otherwise notable about this browser session.</p>"},{"location":"user-guide/browser-profiles/#logging-into-websites","title":"Logging into Websites","text":"<p>To crawl content as a logged in user, log into the website you wish to archive as you would on any other browser. Once the account has been logged in, that's it!</p>"},{"location":"user-guide/browser-profiles/#accepting-popups","title":"Accepting Popups","text":"<p>Some websites are required to get informed consent from users to track them, others require their users to verify their age before viewing adult content. Websites often choose to use cookies \u2014 small pieces of configuration data stored in the browser \u2014 to store this information alongside other cookies such as a login session. Interacting with popups that store the user's choices in a cookie will in turn store those cookies within the browser profile. Like everything else those cookie values will be used when crawling with the browser profile.</p>"},{"location":"user-guide/browser-profiles/#changing-browser-settings","title":"Changing Browser Settings","text":"<p>Browser profiles don't just affect websites! Any of Brave's settings (available at the URL <code>brave://settings/</code>) set in the profile creator will be used while crawling.</p> Example: Blocking page resources with Brave's Shields <p>Whereas the crawler's scoping settings can be used to define which pages should be crawled, Brave's Shields feature can block resources on pages from being loaded. By default, Shields will block EasyList's cookie list but it can be set to block a number of other included lists under Brave <code>Settings &gt; Shields &gt; Filter Lists</code>.</p> <p>Custom Filters can also be useful for blocking sites with resources that aren't blocked by one of the existing lists. We use this at Webrecorder to block our web analytics script while crawling our own website by adding <code>stats.browsertrix.com</code> to the filter list. In this example, <code>browsertrix.com</code> will still load, but Brave will block any communication to <code>stats.browsertrix.com</code> and our analytics won't register a page view as a result. While lots of common analytics tools may already be blocked in an existing blocklist, this one likely isn't because we run it ourselves!</p> <p>The Ublock Origin filter syntax can be used for more specificity over what in-page resources should be blocked.</p> <p>All browser setting related blocking features can be used in addition with the Block Ads by Domain crawler setting.</p>"},{"location":"user-guide/browser-profiles/#editing-existing-browser-profiles","title":"Editing Existing Browser Profiles","text":"<p>Sometimes websites will log users out or expire cookies or login sessions after a period of time. In these cases, when crawling the browser profile can still be loaded but may not behave as it did when it was initially set up.</p> <p>To update the profile, go to the profile's details page and press the Configure Browser Profile button to load and interact with the sites that need to be re-configured. When finished, press the Save Browser Profile button to return to the profile's details page. Profiles are automatically backed up on save if replica storage locations are configured.</p>"},{"location":"user-guide/browser-profiles/#editing-browser-profile-metadata","title":"Editing Browser Profile Metadata","text":"<p>To edit a browser profile's name and description, select Edit Metadata from the actions menu on the profile's details page.</p>"},{"location":"user-guide/collections/","title":"Collections","text":"<p>Collections are the primary way of organizing and combining archived items into groups for presentation.</p> <p>Tip: Combining items from multiple sources</p> <p>If the crawler has not captured every resource or interaction on a webpage, the ArchiveWeb.page browser extension can be used to manually capture missing content and upload it directly to your org.</p> <p>After adding the crawl and the upload to a collection, the content from both will become available in the replay viewer.</p>"},{"location":"user-guide/collections/#adding-content-to-collections","title":"Adding Content to Collections","text":"<p>Crawls and uploads can be added to a collection after creation by selecting Select Archived Items from the collection's actions menu.</p> <p>A crawl workflow can also be set to automatically add any completed archived items to a collection in the workflow's settings.</p>"},{"location":"user-guide/collections/#sharing-collections","title":"Sharing Collections","text":"<p>Collections are private by default, but can be made public by marking them as sharable in the Metadata step of collection creation, or by toggling the Collection is Shareable switch in the share collection dialogue.</p> <p>After a collection has been made public, it can be shared with others using the public URL available in the share collection dialogue. The collection can also be embedded into other websites using the provided embed code. Un-sharing the collection will break any previously shared links.</p> <p>For further resources on embedding archived web content into your own website, see the ReplayWeb.page docs page on embedding.</p>"},{"location":"user-guide/collections/#downloading-collections","title":"Downloading Collections","text":"<p>Downloading a collection will export every archived item within it as a single WACZ file. To download a collection, use the Download Collection option under the collection's Actions dropdown.</p>"},{"location":"user-guide/crawl-workflows/","title":"Crawl Workflows","text":"<p>Crawl Workflows consist of a list of configuration options that instruct the crawler what it should capture.</p>"},{"location":"user-guide/crawl-workflows/#creating-and-editing-crawl-workflows","title":"Creating and Editing Crawl Workflows","text":"<p>New Crawl Workflows can be created from the Crawling page. A detailed breakdown of available settings can be found here.</p>"},{"location":"user-guide/crawl-workflows/#status","title":"Status","text":"<p>Crawl Workflows inherit the status of the last item they created. When a workflow has been instructed to run it can have have five possible states:</p> Status Description  Waiting The workflow can't start running yet but it is queued to run when resources are available.  Starting New resources are starting up. Crawling should begin shortly.  Running The crawler is finding and capturing pages!  Stopping A user has instructed this workflow to stop. Finishing capture of the current pages.  Finishing Crawl The workflow has finished crawling and data is being packaged into WACZ files.  Uploading WACZ WACZ files have been created and are being transferred to storage."},{"location":"user-guide/crawl-workflows/#running-crawl-workflows","title":"Running Crawl Workflows","text":"<p>Crawl workflows can be run from the actions menu of the workflow in the crawl workflow list, or by clicking the Run Crawl button on the workflow's details page.</p> <p>While crawling, the Watch Crawl page displays a list of queued URLs that will be visited, and streams the current state of the browser windows as they visit pages from the queue.</p> <p>Running a crawl workflow that has successfully run previously can be useful to capture content as it changes over time, or to run with an updated Crawl Scope.</p>"},{"location":"user-guide/crawl-workflows/#live-exclusion-editing","title":"Live Exclusion Editing","text":"<p>While exclusions can be set before running a crawl workflow, sometimes while crawling the crawler may find new parts of the site that weren't previously known about and shouldn't be crawled, or get stuck browsing parts of a website that automatically generate URLs known as \"crawler traps\".</p> <p>If the crawl queue is filled with URLs that should not be crawled, use the Edit Exclusions button on the Watch Crawl page to instruct the crawler what pages should be excluded from the queue.</p> <p>Exclusions added while crawling are applied to the same exclusion table saved in the workflow's settings and will be used the next time the crawl workflow is run unless they are manually removed.</p>"},{"location":"user-guide/crawl-workflows/#changing-the-amount-of-crawler-instances","title":"Changing the Amount of Crawler Instances","text":"<p>Like exclusions, the crawler instance scale can also be adjusted while crawling. On the Watch Crawl page, press the Edit Crawler Instances button, and set the desired value.</p> <p>Unlike exclusions, this change will not be applied to future workflow runs.</p>"},{"location":"user-guide/crawl-workflows/#ending-a-crawl","title":"Ending a Crawl","text":"<p>If a crawl workflow is not crawling websites as intended it may be preferable to end crawling operations and update the crawl workflow's settings before trying again. There are two operations to end crawls, available both on the workflow's details page, or as part of the actions menu in the workflow list.</p>"},{"location":"user-guide/crawl-workflows/#stopping","title":"Stopping","text":"<p>Stopping a crawl will throw away the crawl queue but otherwise gracefully end the process and save anything that has been collected. Stopped crawls show up in the list of Archived Items and can be used like any other item in the app.</p>"},{"location":"user-guide/crawl-workflows/#canceling","title":"Canceling","text":"<p>Canceling a crawl will throw away all data collected and immediately end the process. Canceled crawls do not show up in the list of Archived Items, though a record of the runtime and workflow settings can be found in the crawl workflow's list of crawls.</p>"},{"location":"user-guide/org-settings/","title":"Org Settings","text":"<p>The Org Settings page is only available to organization admins. It can be found in the main navigation menu.</p>"},{"location":"user-guide/org-settings/#org-information","title":"Org Information","text":"<p>This tab lets you change the organization's name. This name must be unique.</p>"},{"location":"user-guide/org-settings/#members","title":"Members","text":"<p>This tab lists all current members who have access to the organization, as well as any invited members who have not yet accepted an invitation to join the organization. In the Active Members table, admins can change the permission level of all users in the organization, including other admins. At least one user must be an admin per-organization. Admins can also remove members by pressing the trash button.</p> <p>Admins can add new members to the organization by pressing the Invite New Member button. Enter the email address associated with the user, select the appropriate role, and press Invite to send a link to join the organization via email.</p> <p>Sent invites can be invalidated by pressing the trash button in the relevant Pending Invites table row.</p>"},{"location":"user-guide/org-settings/#permission-levels","title":"Permission Levels","text":"<code>Viewer</code> Users with the viewer role have read-only access to all material within the organization. They cannot create or edit archived items, crawl workflows, browser profiles, or collections. They also do not have access to any crawl analysis or review tools. <code>Crawler</code> Users with the crawler role can create crawl workflows and collections, but they cannot delete existing archived items that they were not responsible for creating. <code>Admin</code> Users with the administrator role have full access to the organization, including its settings page."},{"location":"user-guide/overview/","title":"Org Overview","text":"<p>The overview page delivers key statistics about the organization's resource usage. It also lets users create crawl workflows, uploaded archived items, collections, and browser profiles through the Create New ... button.</p>"},{"location":"user-guide/overview/#storage","title":"Storage","text":"<p>For organizations with a set storage quota, the storage panel displays a visual breakdown of how much space the organization has left and how much has been taken up by all types of archived items and browser profiles. To view additional information about each item, hover over its section in the graph.</p> <p>For organizations with no storage limits the storage panel displays the total size and count of all types of archived items and browser profiles.</p> <p>For all organizations the storage panel displays the total number of archived items.</p>"},{"location":"user-guide/overview/#crawling","title":"Crawling","text":"<p>The crawling panel lists the number of currently running and waiting crawls, as well as the total number of pages captured.</p>"},{"location":"user-guide/overview/#execution-time","title":"Execution Time","text":"<p><code>Paid Feature</code></p> <p>For organizations with a set execution minute limit, the crawling panel displays a graph of how much execution time has been used and how much is currently remaining. Monthly execution time limits reset on the first of each month at 12:00 AM GMT.</p> How is execution time calculated? <p>Execution time is the total runtime of all Crawler Instances during a crawl. For instance, if Crawler Instances scale is set to 2\u00d7 and each crawler instance uses 2 minutes of active crawling time, execution time for the crawl will be 4 minutes. Like elapsed time, this is tracked as the crawl runs so changing the Crawler Instances scale while a crawl is running may change the amount of execution time used in a given time period.</p>"},{"location":"user-guide/overview/#collections","title":"Collections","text":"<p>The collections panel displays the number of total collections and collections marked as sharable.</p>"},{"location":"user-guide/review/","title":"Crawl Review","text":"<p>The Crawl Review page provides a streamlined interface for assessing the capture quality of pages within an archived item using the heuristics collected during crawl analysis.</p> <p>Crawls can only be reviewed once crawl analysis has been run. If multiple analysis runs have been completed, the page analysis heuristics will be used from the selected analysis run, which are displayed next to the archived item name. The most recent analysis run is selected by default, but you can choose to display data from any other completed or stopped analysis run here as well.</p>"},{"location":"user-guide/review/#heuristics","title":"Heuristics","text":"<p>Crawl analysis generates comparisons across three heuristics that can indicate which pages may be the most problematic.</p>"},{"location":"user-guide/review/#screenshot-comparison","title":"Screenshot Comparison","text":"<p>Screenshots are compared by measuring the perceived difference between color samples and by the intensity of difference between pixels. These metrics are provided by the open-source tool Pixelmatch.</p> <p>Discrepancies between crawl and replay screenshots may occur because resources aren't loaded or rendered properly (usually indicating a replay issue).</p> <p>Caveats</p> <p>If many similar pages exhibit similarly poor screenshot comparison scores but look fine in the replay tab, it may be because of page loading time not being long enough during analysis.</p> <p>Some websites may take more time to load than others, including on replay! If the page wasn't given enough time to load during crawl analysis \u2014 because crawl analysis uses the same workflow limit settings as crawling \u2014 increasing the Delay After Page Load workflow setting may yield better screenshot analysis scores, at the cost of extra execution time.</p>"},{"location":"user-guide/review/#extracted-text-comparison","title":"Extracted Text Comparison","text":"<p>Text extracted during crawl analysis is compared to the text extracted during crawling. Text is compared on the basis of Levenshtein distance.</p> <p>Resources not loaded properly on replay may display ReplayWeb.page's <code>Archived Page Not Found</code> error within the extracted text.</p>"},{"location":"user-guide/review/#resource-comparison","title":"Resource Comparison","text":"<p>The resource comparison tab displays a table of resource types, and their HTTP status code count grouped by \"good\" and \"bad\". 2xx &amp; 3xx range status codes are assigned \"good\", 4xx &amp; 5xx range status codes are assigned \"bad\". Bad status codes on crawl indicate that a resource was not successfully captured. Bad status codes on replay that marked good when crawling usually indicate a replay issue.</p> <p>Caveats</p> <p>The number of resources may be higher on replay due to how components of ReplayWeb.page re-write certain request types. A discrepancy alone may not be an indicator that the page is broken, though generally it is a positive sign when the counts are equal.</p> <p>Due to the complicated nature of resource count comparison, this is not available as a sorting option in the pages list.</p>"},{"location":"user-guide/review/#page-review","title":"Page Review","text":"<p>The pages list can be sorted using analysis heuristics to determine the pages that are likely more important to review versus those that might require less attention. After selecting a page to review, looking over the analysis heuristics, and checking them against replay, make a decision about if the page capture was successful or unsuccessful and leave a note about what worked well or what might be problematic.</p> Should I review every page? (Spoiler alert: probably not!) <p>When reviewing a crawl of a site that has many similar pages, all of which exhibit the same error and have similar heuristic scores, it's likely that they all are similarly broken, and you can probably save yourself the trouble. Depending on the website, the heuristic scores may not always be an accurate predictor of quality, but in our testing they are fairly consistent \u2014 consistency being the important factor of this tool. It is up to you, the curator, to make the final quality judgement!</p> <p>Our recommended workflow is as follows: run crawl analysis, examine the most severe issues as highlighted, examine some key examples of common layouts, review any other key pages, and score the crawl accordingly!</p>"},{"location":"user-guide/review/#finish-review","title":"Finish Review","text":"<p>Once a satisfactory amount of pages have been reviewed, press the Finish Review button to give the archived item an overall quality score ranging from \"Excellent!\" to \"Bad\". You can add any additional notes or considerations in the archived item description, which can be edited during this step.</p>"},{"location":"user-guide/signup/","title":"Signup","text":""},{"location":"user-guide/signup/#invite-link","title":"Invite Link","text":"<p>If you have been sent an invite, enter a name and password to create a new account. Your account will be added to the organization you were invited to by an organization admin.</p>"},{"location":"user-guide/signup/#open-registration","title":"Open Registration","text":"<p>If the server has enabled signups and you have been given a registration link, enter your email address, name, and password to create a new account. Your account will be added to the server's default organization.</p>"},{"location":"user-guide/user-settings/","title":"Account Settings","text":""},{"location":"user-guide/user-settings/#display-name","title":"Display Name","text":"<p>This is the name that other users will see as yours in the application.</p>"},{"location":"user-guide/user-settings/#email","title":"Email","text":"<p>This is the email that you use to login. It is also what we'll use to contact you.</p>"},{"location":"user-guide/user-settings/#password","title":"Password","text":"<p>This is the password that you use to login. Passwords must meet a minimum security check. For more information on how we derive password security levels, see zxcvbn.</p>"},{"location":"user-guide/workflow-setup/","title":"Crawl Workflow Setup","text":""},{"location":"user-guide/workflow-setup/#crawl-type","title":"Crawl Type","text":"<p>The first step in creating a new crawl workflow is to choose what type of crawl you want to run. Crawl types are fixed and cannot be converted or changed later.</p> <code>URL List</code> The crawler visits every URL specified in a list, and optionally every URL linked on those pages. <code>Seeded Crawl</code> The crawler automatically discovers and archives pages starting from a single seed URL."},{"location":"user-guide/workflow-setup/#scope","title":"Scope","text":""},{"location":"user-guide/workflow-setup/#list-of-urls","title":"List of URLs","text":"<p><code>URL List</code> <code>Seeded Crawl</code></p> <p>This list informs the crawler what pages it should capture as part of a URL List crawl.</p> <p>It is also available under the Additional URLs section for Seeded Crawls where it can accept arbitrary URLs that will be crawled regardless of other scoping rules.</p>"},{"location":"user-guide/workflow-setup/#include-any-linked-page","title":"Include Any Linked Page","text":"<p><code>URL List</code></p> <p>When enabled, the crawler will visit all the links it finds within each page defined in the List of URLs field.</p> Crawling tags &amp; search queries with URL List crawls <p>This setting can be useful for crawling the content of specific tags or search queries. Specify the tag or search query URL(s) in the List of URLs field, e.g: <code>https://example.com/search?q=tag</code>, and enable Include Any Linked Page to crawl all the content present on that search query page.</p>"},{"location":"user-guide/workflow-setup/#fail-crawl-on-failed-url","title":"Fail Crawl on Failed URL","text":"<p><code>URL List</code></p> <p>When enabled, the crawler will fail the entire crawl if any of the provided URLs are invalid or unsuccessfully crawled. The resulting archived item will have a status of \"Failed\".</p>"},{"location":"user-guide/workflow-setup/#crawl-start-url","title":"Crawl Start URL","text":"<p><code>Seeded Crawl</code></p> <p>This is the first page that the crawler will visit. It's important to set Crawl Start URL that accurately represents the scope of the pages you wish to crawl as the Start URL Scope selection will depend on this field's contents.</p> <p>You must specify the protocol (likely <code>http://</code> or <code>https://</code>) as a part of the URL entered into this field.</p>"},{"location":"user-guide/workflow-setup/#start-url-scope","title":"Start URL Scope","text":"<p><code>Seeded Crawl</code></p> <code>Hashtag Links Only</code> <p>This scope will ignore links that lead to other addresses such as <code>example.com/path</code> and will instead instruct the crawler to visit hashtag links such as <code>example.com/#linkedsection</code>.</p> <p>This scope can be useful for crawling certain web apps that may not use unique URLs for their pages.</p> <code>Pages in the Same Directory</code> This scope will only crawl pages in the same directory as the Crawl Start URL. If <code>example.com/path</code> is set as the Crawl Start URL, <code>example.com/path/path2</code> will be crawled but <code>example.com/path3</code> will not. <code>Pages on This Domain</code> This scope will crawl all pages on the domain entered as the Crawl Start URL however it will ignore subdomains such as <code>subdomain.example.com</code>. <code>Pages on This Domain and Subdomains</code> This scope will crawl all pages on the domain and any subdomains found. If <code>example.com</code> is set as the Crawl Start URL, both pages on <code>example.com</code> and <code>subdomain.example.com</code> will be crawled. <code>Custom Page Prefix</code> This scope will crawl all pages that begin with the Crawl Start URL as well as pages from any URL that begin with the URLs listed in <code>Extra URL Prefixes in Scope</code>"},{"location":"user-guide/workflow-setup/#max-depth","title":"Max Depth","text":"<p><code>Seeded Crawl</code></p> <p>Only shown with a Start URL Scope of <code>Pages on This Domain</code> and above, the Max Depth setting instructs the crawler to stop visiting new links past a specified depth.</p>"},{"location":"user-guide/workflow-setup/#extra-url-prefixes-in-scope","title":"Extra URL Prefixes in Scope","text":"<p><code>Seeded Crawl</code></p> <p>Only shown with a Start URL Scope of <code>Custom Page Prefix</code>, this field accepts additional URLs or domains that will be crawled if URLs that lead to them are found.</p> <p>This can be useful for crawling websites that span multiple domains such as <code>example.org</code> and <code>example.net</code></p>"},{"location":"user-guide/workflow-setup/#include-any-linked-page-one-hop-out","title":"Include Any Linked Page (\"one hop out\")","text":"<p><code>Seeded Crawl</code></p> <p>When enabled, the crawler will visit all the links it finds within each page, regardless of the Start URL Scope setting.</p> <p>This can be useful for capturing links on a page that lead outside the website that is being crawled but should still be included in the archive for context.</p>"},{"location":"user-guide/workflow-setup/#check-for-sitemap","title":"Check For Sitemap","text":"<p><code>Seeded Crawl</code></p> <p>When enabled, the crawler will check for a sitemap at /sitemap.xml and use it to discover pages to crawl if found. It will not crawl pages found in the sitemap that do not meet the crawl's scope settings or limits.</p> <p>This can be useful for discovering and capturing pages on a website that aren't linked to from the seed and which might not otherwise be captured.</p>"},{"location":"user-guide/workflow-setup/#exclusions","title":"Exclusions","text":"<p><code>URL List</code> <code>Seeded Crawl</code></p> <p>The exclusions table will instruct the crawler to ignore links it finds on pages where all or part of the link matches an exclusion found in the table. The table is only available in URL List crawls when Include Any Linked Page is enabled.</p> <p>This can be useful for avoiding crawler traps \u2014 sites that may automatically generate pages such as calendars or filter options \u2014 or other pages that should not be crawled according to their URL.</p> <code>Matches text</code> <p>Will perform simple matching of entered text and exclude all URLs where matching text is found.</p> <p>e.g: If <code>about</code> is entered, <code>example.com/aboutme/</code> will not be crawled.</p> <code>Regex</code> <p>Regular expressions (Regex) can also be used to perform more complex matching.</p> <p>e.g: If <code>\\babout\\/?\\b</code> is entered, <code>example.com/about/</code> will not be crawled however <code>example.com/aboutme/</code> will be crawled.</p>"},{"location":"user-guide/workflow-setup/#limits","title":"Limits","text":""},{"location":"user-guide/workflow-setup/#max-pages","title":"Max Pages","text":"<p>Adds a hard limit on the number of pages that will be crawled. The crawl will be gracefully stopped after this limit is reached.</p>"},{"location":"user-guide/workflow-setup/#crawl-time-limit","title":"Crawl Time Limit","text":"<p>The crawl will be gracefully stopped after this set period of elapsed time.</p>"},{"location":"user-guide/workflow-setup/#crawl-size-limit","title":"Crawl Size Limit","text":"<p>The crawl will be gracefully stopped after reaching this set size in GB.</p>"},{"location":"user-guide/workflow-setup/#crawler-instances","title":"Crawler Instances","text":"<p>Increasing the amount of crawler instances will speed up crawls by using additional browser windows to capture more pages in parallel. This will also increase the amount of traffic sent to the website and may result in a higher chance of getting rate limited.</p>"},{"location":"user-guide/workflow-setup/#page-load-timeout","title":"Page Load Timeout","text":"<p>Limits amount of elapsed time to wait for a page to load. Behaviors will run after this timeout only if the page is partially or fully loaded.</p>"},{"location":"user-guide/workflow-setup/#delay-after-page-load","title":"Delay After Page Load","text":"<p>Waits on the page after initial HTML page load for a set number of seconds prior to moving on to next steps such as link extraction and behaviors. Can be useful with pages that are slow to load page contents.</p>"},{"location":"user-guide/workflow-setup/#behavior-timeout","title":"Behavior Timeout","text":"<p>Limits amount of elapsed time behaviors have to complete.</p>"},{"location":"user-guide/workflow-setup/#auto-scroll-behavior","title":"Auto Scroll Behavior","text":"<p>When enabled, the browser will automatically scroll to the end of the page.</p>"},{"location":"user-guide/workflow-setup/#delay-before-next-page","title":"Delay Before Next Page","text":"<p>Waits on the page for a set period of elapsed time after any behaviors have finished running. This can be helpful to avoid rate limiting however it will slow down your crawl.</p>"},{"location":"user-guide/workflow-setup/#browser-settings","title":"Browser Settings","text":""},{"location":"user-guide/workflow-setup/#browser-profile","title":"Browser Profile","text":"<p>Sets the Browser Profile to be used for this crawl.</p>"},{"location":"user-guide/workflow-setup/#crawler-release-channel","title":"Crawler Release Channel","text":"<p>Sets the release channel of Browsertrix Crawler to be used for this crawl. Crawls started by this workflow will use the latest crawler version from the selected release channel. Generally \"Default\" will be the most stable, however others may have newer features (or bugs)!  </p> <p>This setting will only be shown if multiple different release channels are available for use.</p>"},{"location":"user-guide/workflow-setup/#block-ads-by-domain","title":"Block Ads by Domain","text":"<p>Will prevent any content from the domains listed in Steven Black's Unified Hosts file (ads &amp; malware) from being captured by the crawler.</p>"},{"location":"user-guide/workflow-setup/#user-agent","title":"User Agent","text":"<p>Sets the browser's user agent in outgoing requests to the specified value. If left blank, the crawler will use the Brave browser's default user agent. For a list of common user agents see useragents.me.</p> Using custom user agents to get around restrictions <p>Despite being against best practices, some websites will block specific browsers based on their user agent: a string of text that browsers send web servers to identify what type of browser or operating system is requesting content. If Brave is blocked, using a user agent string of a different browser (such as Chrome or Firefox) may be sufficient to convince the website that a different browser is being used.</p> <p>User agents can also be used to voluntarily identify your crawling activity, which can be useful when working with a website's owners to ensure crawls can be completed successfully. We recommend using a user agent string similar to the following, replacing the <code>orgname</code> and URL comment with your own:</p> <pre><code>Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.3 orgname.browsertrix (+https://example.com/crawling-explination-page)\n</code></pre> <p>If you have no webpage to identify your organization or statement about your crawling activities available as a link, omit the bracketed comment section at the end entirely.</p> <p>This string must be provided to the website's owner so they can allowlist Browsertrix to prevent it from being blocked.</p>"},{"location":"user-guide/workflow-setup/#language","title":"Language","text":"<p>Sets the browser's language setting. Useful for crawling websites that detect the browser's language setting and serve content accordingly.</p>"},{"location":"user-guide/workflow-setup/#scheduling","title":"Scheduling","text":"<p>Tip: Scheduling crawl workflows with logged-in browser profiles</p> <p>Some websites will log users out after a set period of time. When crawling with a custom browser profile that is logged into a website, we recommend checking the profile before crawling to ensure it is still logged in.</p> <p>This can cause issues with scheduled crawl workflows \u2014 which will run even if the selected browser profile has been logged out.</p>"},{"location":"user-guide/workflow-setup/#crawl-schedule-type","title":"Crawl Schedule Type","text":"<code>Run Immediately on Save</code> When selected, the crawl will run immediately as configured. It will not run again unless manually instructed. <code>Run on a Recurring Basis</code> When selected, additional configuration options for instructing the system when to run the crawl will be shown. If a crawl is already running when the schedule is set to activate it, the scheduled crawl will not run. <code>No Schedule</code> When selected, the configuration options that have been set will be saved but the system will not do anything with them unless manually instructed."},{"location":"user-guide/workflow-setup/#frequency","title":"Frequency","text":"<p>Set how often a scheduled crawl will run.</p>"},{"location":"user-guide/workflow-setup/#day","title":"Day","text":"<p>Sets the day of the week for which crawls scheduled with a <code>Weekly</code> Frequency will run.</p>"},{"location":"user-guide/workflow-setup/#date","title":"Date","text":"<p>Sets the date of the month for which crawls scheduled with a <code>Monthly</code> Frequency will run.</p>"},{"location":"user-guide/workflow-setup/#start-time","title":"Start Time","text":"<p>Sets the time that the scheduled crawl will start according to your current timezone.</p>"},{"location":"user-guide/workflow-setup/#also-run-a-crawl-immediately-on-save","title":"Also Run a Crawl Immediately On Save","text":"<p>When enabled, a crawl will run immediately on save as if the <code>Run Immediately on Save</code> Crawl Schedule Type was selected, in addition to scheduling a crawl to run according to the above settings.</p>"},{"location":"user-guide/workflow-setup/#metadata","title":"Metadata","text":""},{"location":"user-guide/workflow-setup/#name","title":"Name","text":"<p>Allows a custom name to be set for the workflow. If no name is set, the workflow's name will be set to the Crawl Start URL. For URL List crawls, the workflow's name will be set to the first URL present in the List of URLs field, with an added <code>(+x)</code> where <code>x</code> represents the total number of URLs in the list.</p>"},{"location":"user-guide/workflow-setup/#description","title":"Description","text":"<p>Leave optional notes about the workflow's configuration.</p>"},{"location":"user-guide/workflow-setup/#tags","title":"Tags","text":"<p>Apply tags to the workflow. Tags applied to the workflow will propagate to every crawl created with it at the time of crawl creation.</p>"},{"location":"user-guide/workflow-setup/#collection-auto-add","title":"Collection Auto-Add","text":"<p>Search for and specify collections that this crawl workflow should automatically add content to as soon as crawling finishes. Canceled and Failed crawls will not be automatically added to collections.</p>"}]}